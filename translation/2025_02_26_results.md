# Cloud times

| Task            | Time   |
|-----------------|--------|
| Create instance | 10m    |
| Start instance  | 1m 30s |

# Results

We are using the Helsinki-NLP/opus-100 (https://huggingface.co/datasets/Helsinki-NLP/opus-100) test set in Arabic, English, Spanish, French and Russian. The results are as follows:

Performance 2000 samples

| Model      | Prompt   | Arabic-English | English-Spanish | English-French | English-Russian |
|------------|----------|----------------|----------------|----------------|-----------------|
| DeepL      |          | 38.00          | -              | 35.73          | 26.94           |
| aya-35b    | Prompt 2 | 31.89          | -              | 32.57          | 22.91           |
| aya-8b     | Prompt 2 | 27.73          | -              | -              | 20              |
| aya-8b     | Prompt 1 | 27.75          | 30.22          | 28.65          | 19.6            |
| command-r  | Prompt 2 | 24.07          | 26.2           | 27.88          | -               |
| llama3-8b  | Prompt 1 | 19.4           | 29.38          | 27.03          | 15.73           |
| gemma2:27b | Prompt 2 | -              | -              | 21.58          | bad             |
| mixtral    | Prompt 2 | no ar          | -              | 18.15          | no rus          |
| llama3.1   | Prompt 2 | -              | 28.77          | 26.28          | -               |



Performance 100 samples

| Model           | Prompt   | Arabic-English | English-Spanish | English-French | English-Russian |
|-----------------|----------|----------------|-----------------|----------------|-----------------|
| DeepL           |          | 33.11          | -               | 36.05          | 24.64           |
| llama3.1:70B    | Prompt 3 | -              | -               | 32.65          | -               |
| aya-35b         | Prompt 2 | 30.75          | -               | 31.48          | 20.06           |
| aya-expanse:32b | Prompt 3 | 19.26          | 19.82           | 23.92          | 15.3            |
| glm4:9b         | Prompt 2 | 19.62          | -               | 30.21          | 16.12           |
| glm-BF16-64     | Prompt 2 | 18.75          | -               | 28.84          | 17.20           |
| glm-BF16-128    | Prompt 2 | 20.05          | -               | 30.09          | 17.82           |
| llama3.1-8B     | Prompt 2 | 10.52          | 25.37           | 27.53          | 14.04           |
| llama3.1-8B     | Prompt 3 | -              | -               | 26.57          | -               |
| llama3.2-3B     | Prompt 3 | -              | -               | 19.70          | -               |




## GPU Performance Comparison

| Setup                    | Iteration 1 (seconds)                 | Iteration 2 (seconds) | Iteration 3 (seconds) | Total Time for All Rounds (seconds) |
|--------------------------|---------------------------------------|-----------------------|-----------------------|-------------------------------------|
| **1 x NVIDIA L4 (1 x 24 GB)** | 758.91 (including model loading time) | 599.78                | 617.81                | 1976.5                              |
| **2 x NVIDIA T4 (2x16 GB)** | 781.4 (including model loading time)  | 731.8                 | 697.23                | 2210.42                             |


Speed

| Model           | 1 sentence |
|-----------------|------------|
| DeepL           | 0.4s       |
| llama3-8b       | 0.86s      |
| aya-8b          | 0.925s     |
| aya-35b         | 3.3s       |
| aya-expanse:32b | 2.14s      |
| gemma2:27b      | 1.4s       |
| mixtral         | 2.5s       |
| glm4:9b         | 0.4s       |




# BLEU Score

| BLEU SCORE | INTERPRETATION                                       |
|------------|------------------------------------------------------|
| < 10       | Almost useless                                       |
| 10 - 19    | Hard to get the gist                                 |
| 20 - 29    | The gist is clear, but has significant errors        |
| 30 - 40    | Understandable to good translations                  |
| 40 - 50    | High quality translations                            |
| 50 - 60    | Very high quality, adequate, and fluent translations |
| > 60       | Quality often better than human                      |